{
    "docs": [
        {
            "location": "/",
            "text": "About the Linked Data Competency Index (LDCI)\n\n\nThe \nLinked Data Competency Index\n, or LDCI, is a set of topically arranged assertions of the knowledge, skills, and habits of mind required for professional practice in the area of Linked Data.  The LDCI provides an overview, or map, of the Linked Data field both for independent learners who want to learn Linked Data methods and technology, and for professors or trainers who want to design and teach courses on the subject.  \n\n\nThe assertions enumerated in the LDCI are identified with unique, Web-based identifiers (URIs) that can be used to tag corresponding resources, from books or Web-based tutorials to YouTube videos, for indexing and retrieval.\n\n\nThe LDCI is the product of two projects: the \nLearning Linked Data Project\n, twelve-month planning activity funded in 2011 and 2012 by the \nInstitute of Museum and Library Services (IMLS)\n to explore the creation of a software platform for learning to interpret and create Linked Data, and its follow-up project, \nLD4PE\n, or Linked Data for Professional Education, funded by IMLS from 2014 to 2017.  \n\n\nTo create this index, the LD4PE Competency Index Editorial Board devised its own \nstylistic principles\n.  The Board strove for stylistic coherence, consistent granularity, manageable length, and general readability.  The goal was to formulate a Competency Index that could be printed out by interested learners and profitably be read from start to finish, with language evocative enough to convey the general drift of the subject matter even to newcomers.\n\n\nA living document, the index is now maintained by the Dublin Core Metadata Initiative through crowdsourcing.  Interested users can contribute to its improvement by forking the \nLDCI Github repository\n, making edits to your copy of the index, and submitting those edits to the Competency Index Editorial Board as a pull request.  Due to the excellent integration of \nMkDocs\n, the software used to generate this website, with Github, this process is actually \nmuch easier than it sounds\n!",
            "title": "About"
        },
        {
            "location": "/#about-the-linked-data-competency-index-ldci",
            "text": "The  Linked Data Competency Index , or LDCI, is a set of topically arranged assertions of the knowledge, skills, and habits of mind required for professional practice in the area of Linked Data.  The LDCI provides an overview, or map, of the Linked Data field both for independent learners who want to learn Linked Data methods and technology, and for professors or trainers who want to design and teach courses on the subject.    The assertions enumerated in the LDCI are identified with unique, Web-based identifiers (URIs) that can be used to tag corresponding resources, from books or Web-based tutorials to YouTube videos, for indexing and retrieval.  The LDCI is the product of two projects: the  Learning Linked Data Project , twelve-month planning activity funded in 2011 and 2012 by the  Institute of Museum and Library Services (IMLS)  to explore the creation of a software platform for learning to interpret and create Linked Data, and its follow-up project,  LD4PE , or Linked Data for Professional Education, funded by IMLS from 2014 to 2017.    To create this index, the LD4PE Competency Index Editorial Board devised its own  stylistic principles .  The Board strove for stylistic coherence, consistent granularity, manageable length, and general readability.  The goal was to formulate a Competency Index that could be printed out by interested learners and profitably be read from start to finish, with language evocative enough to convey the general drift of the subject matter even to newcomers.  A living document, the index is now maintained by the Dublin Core Metadata Initiative through crowdsourcing.  Interested users can contribute to its improvement by forking the  LDCI Github repository , making edits to your copy of the index, and submitting those edits to the Competency Index Editorial Board as a pull request.  Due to the excellent integration of  MkDocs , the software used to generate this website, with Github, this process is actually  much easier than it sounds !",
            "title": "About the Linked Data Competency Index (LDCI)"
        },
        {
            "location": "/D2695955/",
            "text": "LD4PE Competency Index\n\n\nVersion: 2017-06-28 14:34:35 \n\nView at: \nhttps://dcmi.github.io/ldci/D2695955/\n \n\n\n\n\n\n\n\n\nCode\n\n\nType\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nA\n\n\nTopic Cluster\n\n\n\n\n\n\n\n\nB\n\n\nTopic\n\n\n\n\n\n\n\n\nC\n\n\nCompetency\n\n\nTweet-length assertion of knowledge, skill, or habit of mind.\n\n\n\n\n\n\nD\n\n\nBenchmark\n\n\nAction demonstrating accomplishment in related competencies.\n\n\n\n\n\n\n\n\nNote: Hover over a code to see its URI.  Click on a code to visit its full definition on the \nAchievement Standards Network\n website.\n\n\nA:\n Fundamentals of Resource Description Framework\n\n\n\n\nB:\n Identity in RDF\n\n\nC:\n Knows that anything can be named with Uniform Resource Identifiers (URIs), such as agents, places, events, artifacts, and concepts.\n\n\nC:\n Understands that a \"real-world\" thing may need to be named with a URI distinct from the URI for information about that thing.\n\n\nC:\n Recognizes that URIs are \"owned\" by the owners of their respective Internet domains.\n\n\nC:\n Knows that Uniform Resource Identifiers, or URIs (1994), include Uniform Resource Locators (URLs, which locate web pages) as well as location-independent identifiers for physical, conceptual, or web resources.\n\n\n\n\n\n\nB:\n RDF data model\n\n\nC:\n Knows the subject-predicate-object component structure of a triple.\n\n\nC:\n Understands the difference between literals and non-literal resources.\n\n\nC:\n Understands that URIs and literals denote things in the world (\"resources\") real, imagined, or conceptual.\n\n\nC:\n Understands that resources are declared to be members (instances) of classes using the property rdf:type.\n\n\nC:\n Understands the use of datatypes and language tags with literals.\n\n\nC:\n Understands blank nodes and their uses.\n\n\nC:\n Understands that QNames define shorthand prefixes for long URIs.\n\n\nD:\n Uses prefixes for URIs in RDF specifications and data.\n\n\n\n\n\n\nC:\n Articulates differences between the RDF abstract data model and the XML and relational models.\n\n\nC:\n Understands the RDF abstract data model as a directed labeled graph.\n\n\nC:\n Knows graphic conventions for depicting RDF-based models.\n\n\nD:\n Can use graphing or modeling software to share those models with others.\n\n\n\n\n\n\nC:\n Understands a named graph as one of the collection of graphs comprising an RDF dataset, with a graph name unique in the context of that dataset.\n\n\nC:\n Understands how a namespace, informally used in the RDF context for a namespace URI or RDF vocabulary, fundamentally differs from the namespace of data attributes and functions (methods) defined for an object-oriented class.\n\n\n\n\n\n\nB:\n Related data models\n\n\nC:\n Grasps essential differences between schemas for syntactic validation (e.g., XML) and for inferencing (RDF Schema).\n\n\nC:\n Differentiates hierarchical document models (eg, XML) and graph models (RDF).\n\n\nC:\n Understands how an RDF class (named set of things) fundamentally differs from an object-oriented programming class, which defines a type of object bundling \"state\" (attributes with data values) and \"behavior\" (functions that operate on state).\n\n\n\n\n\n\nB:\n RDF serialization\n\n\nC:\n Understands RDF serializations as interchangeable encodings of a given set of triples (RDF graph).\n\n\nD:\n Uses tools to convert RDF data between different serializations.\n\n\n\n\n\n\nC:\n Distinguishes the RDF abstract data model and concrete serializations of RDF data.\n\n\nD:\n Expresses data in serializations such as RDF/XML, N-Triples, Turtle, N3, Trig, JSON-LD, and RDFa.\n\n\n\n\n\n\n\n\n\n\n\n\nA:\n Fundamentals of Linked Data\n\n\n\n\nB:\n Web technology\n\n\nC:\n Knows the origins of the World Wide Web (1989) as a non-linear interactive system, or hypermedia, built on the Internet.\n\n\nC:\n Understands that Linked Data (2006) extended the notion of a web of documents (the Web) to a notion of a web of finer-grained data (the Linked Data cloud).\n\n\nC:\n Knows HyperText Markup Language, or HTML (1991+), as a language for \"marking up\" the content and multimedia components of Web pages.\n\n\nC:\n Knows HTML5 (2014) as a version of HTML extended with support for complex web and mobile applications.\n\n\nC:\n Knows Hypertext Transfer Protocol, or HTTP (1991+), as the basic technology for resolving hyperlinks and transferring data on the World Wide Web.\n\n\nC:\n Knows Representational State Transfer, or REST (2000) as a software architectural style whereby browsers can exchange data with web servers, typically on the basis of well-known HTTP actions.\n\n\n\n\n\n\nB:\n Linked Data principles\n\n\nC:\n Knows Tim Berners-Lee's principles of Linked Data: use URIs to name things, use HTTP URIs that can be resolved to useful information, and create links to URIs of other things.\n\n\nC:\n Knows the \"five stars\" of Open Data: put data on the Web, preferably in a structured and preferably non-proprietary format, using URIs to name things, and link to other data.\n\n\n\n\n\n\nB:\n Linked Data policies and best practices\n\n\nC:\n Knows the primary organizations related to Linked Data standardization.\n\n\nD:\n Participates in developing standards and best practice with relevant organizations such as W3C.\n\n\n\n\n\n\n\n\n\n\nB:\n Non-RDF linked data\n\n\n\n\nA:\n RDF vocabularies and application profiles\n\n\n\n\nB:\n Finding RDF-based vocabularies\n        * \nD:\n [MOVE] Knows portals and registries for finding RDF-based vocabularies.\n        * \nD:\n Finds properties and classes in the Linked Open Vocabularies (LOV) observatory and explores their versions and dependencies.\n\n\nB:\n Designing RDF-based vocabularies\n\n\nC:\n Uses RDF Schema to express semantic relationships within a vocabulary.\n\n\nD:\n Correctly uses sub-class relationships in support of inference.\n\n\nD:\n Correctly uses sub-property relationships in support of inference.\n\n\n\n\n\n\nC:\n Reuses published properties and classes where available.\n\n\nC:\n Coins namespace URIs, as needed, for any new properties and classes required.\n\n\nD:\n Drafts a policy for coining URIs for properties and classes.\n\n\nD:\n Chooses \"hash\"- or \"slash\"-based URI patterns based on requirements.\n\n\n\n\n\n\nC:\n Knows Web Ontology Language, or OWL (2004), as a RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).\n\n\nC:\n Knows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain.\n\n\nC:\n Knows Simple Knowledge Organization System, or SKOS (2009), an RDF vocabulary for expressing concepts that are labeled in natural languages, organized into informal hierarchies, and aggregated into concept schemes.\n\n\nC:\n Knows SKOS eXtension for Labels, or SKOS-XL (2009), a small set of additional properties for describing and linking lexical labels as instances of the class Label.\n\n\nC:\n Understands that in a formal sense, a SKOS concept is not an RDF class but an instance and, as such, is not formally associated with a set of instances (\"class extension\").\n\n\nC:\n Understands that SKOS can express a flexibly associative structure of concepts without enabling the more rigid and automatic inferences typically specified in a class-based OWL ontology.\n\n\nC:\n Understands that in contrast to OWL sub-class chains, hierarchies of SKOS concepts are designed not to form transitive chains automatically because this is not how humans think or organize information.\n\n\nC:\n Knows the naming conventions for RDF properties and classes.\n\n\n\n\n\n\nB:\n Maintaining RDF vocabularies\n\n\nC:\n Understands policy options for persistence guarantees.\n\n\nD:\n Can draft a persistence policy.\n\n\n\n\n\n\n\n\n\n\nB:\n Versioning RDF vocabularies\n\n\nC:\n Knows technical options for the form, content, and granularity of versions.\n\n\nC:\n Understands the trade-offs between publishing RDF vocabularies in periodic, numbered releases versus more continual or incremental approaches.\n\n\nD:\n Can express and justify a versioning policy.\n\n\n\n\n\n\n\n\n\n\nB:\n Publishing RDF vocabularies\n\n\nC:\n Understands the typical publication formats for RDF vocabularies and their relative advantages\n\n\nC:\n Understands the purpose of publishing RDF vocabularies in multiple formats using content negotiation.\n\n\nC:\n Understands that to be \"dereferencable\", a URI should be usable to retrieve a representation of the resource it identifies.\n\n\nD:\n Ensures that when dereferenced by a Web browser, a URI returns a representation of the resource in human-readable HTML.\n\n\nD:\n Ensures that when dereferenced by an RDF application, a URI returns representation of the resource in the requested RDF serialization syntax.\n\n\n\n\n\n\n\n\n\n\nB:\n Mapping RDF vocabularies\n\n\nC:\n Understands that the properties of hierarchical subsumption within an RDF vocabulary -- rdfs:subPropertyOf and rdfs:subClassOf -- can also be used to express mappings between vocabularies.\n\n\nC:\n Understands that owl:equivalentProperty and owl:equivalentClass may be used when equivalencies between properties or between classes are exact.\n\n\nC:\n Recognizes that owl:sameAs, while popular as a mapping property, has strong formal semantics that can entail unintended inferences.\n\n\n\n\n\n\nB:\n RDF application profiles\n\n\nC:\n Identifies real-world entities in an application domain as candidates for RDF classes.\n\n\nC:\n Identifies resource attributes and relationships between domain entities as candidates for RDF properties.\n\n\nC:\n Investigates how others have modeled the same or similar application domains.\n\n\nD:\n Communicates a domain model with words and diagrams.\n\n\nD:\n Participates in the social process of developing application profiles.\n\n\n\n\n\n\n\n\n\n\n\n\nA:\n Creating and transforming Linked Data\n\n\n\n\nB:\n Managing identifiers (URI)\n\n\nC:\n Understands that to be \"persistent\", a URI must have a stable, well-documented meaning and be plausibly intended to identify a given resource in perpetuity.\n\n\nC:\n Understands trade-offs between \"opaque\" URIs and URIs using version numbers, server names, dates, application-specific file extensions, query strings or other obsoletable context.\n\n\nC:\n Recognizes the desirability of a published namespace policy describing an institution's commitment to the persistence and semantic stability of important URIs.\n\n\n\n\n\n\nB:\n Creating RDF data\n\n\nC:\n Generates RDF data from non-RDF sources.\n\n\nC:\n Knows methods for generating RDF data from tabular data in formats such as Comma-Separated Values (CSV).\n\n\nC:\n Knows methods such as Direct Mapping of Relational Data to RDF (2012) for transforming data from the relational model (keys, values, rows, columns, tables) into RDF graphs.\n\n\n\n\n\n\nB:\n Versioning RDF data\n\n\nB:\n RDF data provenance\n\n\nB:\n Cleaning and reconciling RDF data\n\n\nC:\n Cleans a dataset by finding and correcting errors, removing duplicates and unwanted data.\n\n\n\n\n\n\nB:\n Mapping and enriching RDF data\n\n\nC:\n Uses available resources for named entity recognition, extraction, and reconciliation.\n\n\n\n\n\n\n\n\nA:\n Interacting with RDF data\n\n\n\n\nB:\n Finding RDF data\n\n\nC:\n Knows relevant resources for discovering existing Linked Data datasets.\n\n\nC:\n Retrieves and accesses RDF data from the \"open Web\".\n\n\nC:\n Monitors and updates lists which report the status of SPARQL endpoints.\n\n\nC:\n Uses available vocabularies for dataset description to support their discovery.\n\n\nC:\n Registers datasets with relevant services for discovery.\n\n\n\n\n\n\nB:\n Processing RDF data using programming languages.\n\n\nC:\n Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.\n\n\nD:\n Uses an RDF programming library to serialize RDF data in available syntaxes.\n\n\nD:\n Uses RDF-specific programming methods to iterate over components of RDF data.\n\n\nD:\n Uses RDF-library-specific convenience representations for common RDF vocabularies such as RDF, Dublin Core, and SKOS.\n\n\n\n\n\n\nC:\n Programatically associates namespaces to prefixes for use in serializing RDF or when parsing SPARQL queries.\n\n\nD:\n Uses RDF programming libraries to extract RDF data from CSV files, databases, or web pages.\n\n\nD:\n Uses RDF programming libraries to persistently stores triples in memory, on disk, or to interact with triple stores.\n\n\nD:\n Programatically infers triples using custom functions or methods.\n\n\n\n\n\n\nC:\n Understands how the pattern matching of SPARQL queries can be expressed using functionally equivalent constructs in RDF programming libraries.\n\n\nD:\n Uses RDF-specific programming methods to query RDF data and save the results for further processing.\n\n\nD:\n Uses utilities and convenience functions the provide shortcuts for frequently used patterns, such as matching the multiple label properties used in real data.\n\n\nD:\n Uses RDF libraries to process various types of SPARQL query result.\n\n\n\n\n\n\n\n\n\n\nB:\n Querying RDF data\n\n\nC:\n Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.\n\n\nC:\n Understands the basic syntax of a SPARQL query.\n\n\nD:\n Uses angle brackets for delimiting URIs.\n\n\nD:\n Uses question marks for indicating variables.\n\n\nD:\n Uses PREFIX for base URIs.\n\n\n\n\n\n\nC:\n Demonstrates a working knowledge of the forms and uses of SPARQL result sets (SELECT, CONSTRUCT, DESCRIBE, and ASK).\n\n\nD:\n Uses the SELECT clause to identify the variables to appear in a table of query results.\n\n\nD:\n Uses the WHERE clause to provide the graph pattern to match against the graph data.\n\n\nD:\n Uses variables in SELECT and WHERE clauses to yield a table of results.\n\n\nD:\n Uses ASK for a True/False result test for a match to a query pattern.\n\n\nD:\n Uses DESCRIBE to extract a single graph containing RDF data about resources.\n\n\nD:\n Uses CONSTRUCT to extract and transform results into a single RDF graph specified by a graph template.\n\n\nD:\n Uses FROM to formulate queries with URLs and local files.\n\n\n\n\n\n\nC:\n Understands how to combine and filter graph patterns using operators such as UNION, OPTIONAL, FILTER, and MINUS.\n\n\nD:\n Uses UNION to formulate queries with multiple possible graph patterns.\n\n\nD:\n Uses OPTIONAL to formulate queries to return the values of optional variables when available.\n\n\nD:\n Uses FILTER to formulates queries that eliminate solutions from a result set.\n\n\nD:\n Uses NOT EXISTS to limit whether a given graph pattern exists in the data.\n\n\nD:\n Uses MINUS to remove matches from a result based on the evaluation of two patterns.\n\n\nD:\n Uses NOT IN to restrict a variable to not being in a given set of values.\n\n\n\n\n\n\nC:\n Understands the major SPARQL result set modifiers, e.g., to limit or sort results, or to return distinct results only once.\n\n\nD:\n Uses ORDER BY to define ordering conditions by variable, function call, or expression.\n\n\nD:\n Uses DISTINCT to ensure solutions in the sequence are unique.\n\n\nD:\n Uses OFFSET to control where the solutions processed start in the overall sequence of solutions.\n\n\nD:\n Uses LIMIT to restrict the number of solutions processed for query results.\n\n\nD:\n Uses projection to transform a solution sequence into one involving only a subset of the variables.\n\n\n\n\n\n\nC:\n Understands the use of SPARQL functions and operators.\n\n\nD:\n Uses the regular expression (regex()) function for string matching.\n\n\nD:\n Uses aggregates to apply expressions over groups of solutions (GROUP BY, COUNT, SUM, AVG, MIN) for partitioning results, evaluating projections, and filtering.\n\n\nD:\n Uses the lang() function to return the language tag of an RDF literal.\n\n\nD:\n Uses the langMatches() function to match a language tag against a language range.\n\n\nD:\n Uses the xsd:decimal(expn) function to convert an expression to an integer.\n\n\nD:\n Uses the GROUP BY clause to transforms a result set so that only one row will appear for each unique set of grouping variables.\n\n\nD:\n Uses the HAVING clause to apply a filter to the result set after grouping.\n\n\n\n\n\n\nC:\n Differentiates between a Default Graph and a Named Graph, and formulates queries using the GRAPH clause.\n\n\nD:\n Formulates advanced queries using FROM NAMED and GRAPH on local data.\n\n\nD:\n Formulates advanced queries using FROM NAMED on remote data.\n\n\nD:\n Formulates advanced queries on data containing blank nodes.\n\n\nD:\n Formulates advanced queries using subqueries.\n\n\n\n\n\n\nC:\n Uses a temporary variable to extend a query.\n\n\nC:\n Understands the role of Property Paths and how they are formed by combining predicates with regular expression-like operators.\n\n\nC:\n Understands the concept of Federated Searches.\n\n\nD:\n Formulates advanced queries on a remote SPARQL endpoint using the SERVICE directive.\n\n\nD:\n Uses federated query to query over a local graph store and one or more other SPARQL endpoints.\n\n\nD:\n Pulls data from a different SPARQL endpoints in one single query using the SERVICE directive.\n\n\n\n\n\n\nC:\n Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs.\n\n\nC:\n Reads and understands high-level descriptions of the classes and properties of a dataset in order to write queries.\n\n\nC:\n Uses available tools, servers, and endpoints to issue queries against a dataset.\n\n\nD:\n Execute SPARQL queries using the Jena ARQ command-line utility.\n\n\nD:\n Queries multiple local data files using ARQ.\n\n\nD:\n Uses ARQ to evaluate queries on local data.\n\n\nD:\n Uses Fuseki server to evaluate queries on a dataset.\n\n\nD:\n Queries multiple data files using Fuseki.\n\n\nD:\n Accesses DBPedia's SNORQL/SPARQL endpoint and issues simple queries.\n\n\n\n\n\n\n\n\n\n\nB:\n Visualizing RDF data\n\n\nC:\n Uses publicly available tools to visualize data.\n\n\nD:\n Uses Google FusionTables to create maps and charts.\n\n\n\n\n\n\nC:\n Distills results taken from large datasets so that visualizations are human-friendly.\n\n\nC:\n Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by third party tools and APIs.\n\n\n\n\n\n\nB:\n Reasoning over RDF data\n\n\nC:\n Understands the principles and practice of inferencing.\n\n\nC:\n Uses common entailment regimes and understands their uses.\n\n\nC:\n Understands the role of formally declared domains and ranges for inferencing.\n\n\nC:\n Understands how reasoning can be used for integrating diverse datasets.\n\n\nC:\n Knows that Web Ontology Language (OWL) is available in multiple \"flavors\" that are variously optimized for expressivity, performant reasoning, or for applications involving databases or business rules.\n\n\nC:\n Understands that OWL Full supports all available constructs and is most appropriately used when reasoning performance is not a concern.\n\n\n\n\n\n\nB:\n Assessing RDF data quality\n\n\nB:\n RDF data analytics\n\n\nC:\n Uses available ontology browsing tools to explore the ontologies used in a particular dataset.\n\n\n\n\n\n\nB:\n Manipulating RDF data\n\n\nC:\n Knows the SPARQL 1.1 Update language for updating, creating, and removing RDF graphs in a Graph Store\n\n\nD:\n Uses INSERT/DELETE to update triples.\n\n\nD:\n Uses a CONSTRUCT query to preview changes before executing an INSERT/DELETE operation.\n\n\n\n\n\n\nC:\n Knows the SPARQL 1.1 Graph Store HTTP protocol for updating graphs on a web server (in \"restful\" style).\n\n\nD:\n Uses GET to retrieve triples from a default graph or a named graph.\n\n\nD:\n Uses PUT to insert set of triples into a new graph (or replace an existing graph).\n\n\nD:\n Uses DELETE to remove a graph.\n\n\nD:\n Uses POST to add triples to an existing graph.\n\n\nD:\n Uses proper syntax to request specific media types, such as Turtle.\n\n\n\n\n\n\nC:\n Understands the difference between SQL query language (which operates on database tables) and SPARQL (which operates on RDF graphs).\n\n\n\n\n\n\n\n\nA:\n Creating Linked Data applications\n\n\n\n\nB:\n Storing RDF data",
            "title": "The Index"
        },
        {
            "location": "/D2695955/#ld4pe-competency-index",
            "text": "Version: 2017-06-28 14:34:35  \nView at:  https://dcmi.github.io/ldci/D2695955/       Code  Type  Definition      A  Topic Cluster     B  Topic     C  Competency  Tweet-length assertion of knowledge, skill, or habit of mind.    D  Benchmark  Action demonstrating accomplishment in related competencies.     Note: Hover over a code to see its URI.  Click on a code to visit its full definition on the  Achievement Standards Network  website.",
            "title": "LD4PE Competency Index"
        },
        {
            "location": "/D2695955/#a-fundamentals-of-resource-description-framework",
            "text": "B:  Identity in RDF  C:  Knows that anything can be named with Uniform Resource Identifiers (URIs), such as agents, places, events, artifacts, and concepts.  C:  Understands that a \"real-world\" thing may need to be named with a URI distinct from the URI for information about that thing.  C:  Recognizes that URIs are \"owned\" by the owners of their respective Internet domains.  C:  Knows that Uniform Resource Identifiers, or URIs (1994), include Uniform Resource Locators (URLs, which locate web pages) as well as location-independent identifiers for physical, conceptual, or web resources.    B:  RDF data model  C:  Knows the subject-predicate-object component structure of a triple.  C:  Understands the difference between literals and non-literal resources.  C:  Understands that URIs and literals denote things in the world (\"resources\") real, imagined, or conceptual.  C:  Understands that resources are declared to be members (instances) of classes using the property rdf:type.  C:  Understands the use of datatypes and language tags with literals.  C:  Understands blank nodes and their uses.  C:  Understands that QNames define shorthand prefixes for long URIs.  D:  Uses prefixes for URIs in RDF specifications and data.    C:  Articulates differences between the RDF abstract data model and the XML and relational models.  C:  Understands the RDF abstract data model as a directed labeled graph.  C:  Knows graphic conventions for depicting RDF-based models.  D:  Can use graphing or modeling software to share those models with others.    C:  Understands a named graph as one of the collection of graphs comprising an RDF dataset, with a graph name unique in the context of that dataset.  C:  Understands how a namespace, informally used in the RDF context for a namespace URI or RDF vocabulary, fundamentally differs from the namespace of data attributes and functions (methods) defined for an object-oriented class.    B:  Related data models  C:  Grasps essential differences between schemas for syntactic validation (e.g., XML) and for inferencing (RDF Schema).  C:  Differentiates hierarchical document models (eg, XML) and graph models (RDF).  C:  Understands how an RDF class (named set of things) fundamentally differs from an object-oriented programming class, which defines a type of object bundling \"state\" (attributes with data values) and \"behavior\" (functions that operate on state).    B:  RDF serialization  C:  Understands RDF serializations as interchangeable encodings of a given set of triples (RDF graph).  D:  Uses tools to convert RDF data between different serializations.    C:  Distinguishes the RDF abstract data model and concrete serializations of RDF data.  D:  Expresses data in serializations such as RDF/XML, N-Triples, Turtle, N3, Trig, JSON-LD, and RDFa.",
            "title": "A: Fundamentals of Resource Description Framework"
        },
        {
            "location": "/D2695955/#a-fundamentals-of-linked-data",
            "text": "B:  Web technology  C:  Knows the origins of the World Wide Web (1989) as a non-linear interactive system, or hypermedia, built on the Internet.  C:  Understands that Linked Data (2006) extended the notion of a web of documents (the Web) to a notion of a web of finer-grained data (the Linked Data cloud).  C:  Knows HyperText Markup Language, or HTML (1991+), as a language for \"marking up\" the content and multimedia components of Web pages.  C:  Knows HTML5 (2014) as a version of HTML extended with support for complex web and mobile applications.  C:  Knows Hypertext Transfer Protocol, or HTTP (1991+), as the basic technology for resolving hyperlinks and transferring data on the World Wide Web.  C:  Knows Representational State Transfer, or REST (2000) as a software architectural style whereby browsers can exchange data with web servers, typically on the basis of well-known HTTP actions.    B:  Linked Data principles  C:  Knows Tim Berners-Lee's principles of Linked Data: use URIs to name things, use HTTP URIs that can be resolved to useful information, and create links to URIs of other things.  C:  Knows the \"five stars\" of Open Data: put data on the Web, preferably in a structured and preferably non-proprietary format, using URIs to name things, and link to other data.    B:  Linked Data policies and best practices  C:  Knows the primary organizations related to Linked Data standardization.  D:  Participates in developing standards and best practice with relevant organizations such as W3C.      B:  Non-RDF linked data",
            "title": "A: Fundamentals of Linked Data"
        },
        {
            "location": "/D2695955/#a-rdf-vocabularies-and-application-profiles",
            "text": "B:  Finding RDF-based vocabularies\n        *  D:  [MOVE] Knows portals and registries for finding RDF-based vocabularies.\n        *  D:  Finds properties and classes in the Linked Open Vocabularies (LOV) observatory and explores their versions and dependencies.  B:  Designing RDF-based vocabularies  C:  Uses RDF Schema to express semantic relationships within a vocabulary.  D:  Correctly uses sub-class relationships in support of inference.  D:  Correctly uses sub-property relationships in support of inference.    C:  Reuses published properties and classes where available.  C:  Coins namespace URIs, as needed, for any new properties and classes required.  D:  Drafts a policy for coining URIs for properties and classes.  D:  Chooses \"hash\"- or \"slash\"-based URI patterns based on requirements.    C:  Knows Web Ontology Language, or OWL (2004), as a RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).  C:  Knows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain.  C:  Knows Simple Knowledge Organization System, or SKOS (2009), an RDF vocabulary for expressing concepts that are labeled in natural languages, organized into informal hierarchies, and aggregated into concept schemes.  C:  Knows SKOS eXtension for Labels, or SKOS-XL (2009), a small set of additional properties for describing and linking lexical labels as instances of the class Label.  C:  Understands that in a formal sense, a SKOS concept is not an RDF class but an instance and, as such, is not formally associated with a set of instances (\"class extension\").  C:  Understands that SKOS can express a flexibly associative structure of concepts without enabling the more rigid and automatic inferences typically specified in a class-based OWL ontology.  C:  Understands that in contrast to OWL sub-class chains, hierarchies of SKOS concepts are designed not to form transitive chains automatically because this is not how humans think or organize information.  C:  Knows the naming conventions for RDF properties and classes.    B:  Maintaining RDF vocabularies  C:  Understands policy options for persistence guarantees.  D:  Can draft a persistence policy.      B:  Versioning RDF vocabularies  C:  Knows technical options for the form, content, and granularity of versions.  C:  Understands the trade-offs between publishing RDF vocabularies in periodic, numbered releases versus more continual or incremental approaches.  D:  Can express and justify a versioning policy.      B:  Publishing RDF vocabularies  C:  Understands the typical publication formats for RDF vocabularies and their relative advantages  C:  Understands the purpose of publishing RDF vocabularies in multiple formats using content negotiation.  C:  Understands that to be \"dereferencable\", a URI should be usable to retrieve a representation of the resource it identifies.  D:  Ensures that when dereferenced by a Web browser, a URI returns a representation of the resource in human-readable HTML.  D:  Ensures that when dereferenced by an RDF application, a URI returns representation of the resource in the requested RDF serialization syntax.      B:  Mapping RDF vocabularies  C:  Understands that the properties of hierarchical subsumption within an RDF vocabulary -- rdfs:subPropertyOf and rdfs:subClassOf -- can also be used to express mappings between vocabularies.  C:  Understands that owl:equivalentProperty and owl:equivalentClass may be used when equivalencies between properties or between classes are exact.  C:  Recognizes that owl:sameAs, while popular as a mapping property, has strong formal semantics that can entail unintended inferences.    B:  RDF application profiles  C:  Identifies real-world entities in an application domain as candidates for RDF classes.  C:  Identifies resource attributes and relationships between domain entities as candidates for RDF properties.  C:  Investigates how others have modeled the same or similar application domains.  D:  Communicates a domain model with words and diagrams.  D:  Participates in the social process of developing application profiles.",
            "title": "A: RDF vocabularies and application profiles"
        },
        {
            "location": "/D2695955/#a-creating-and-transforming-linked-data",
            "text": "B:  Managing identifiers (URI)  C:  Understands that to be \"persistent\", a URI must have a stable, well-documented meaning and be plausibly intended to identify a given resource in perpetuity.  C:  Understands trade-offs between \"opaque\" URIs and URIs using version numbers, server names, dates, application-specific file extensions, query strings or other obsoletable context.  C:  Recognizes the desirability of a published namespace policy describing an institution's commitment to the persistence and semantic stability of important URIs.    B:  Creating RDF data  C:  Generates RDF data from non-RDF sources.  C:  Knows methods for generating RDF data from tabular data in formats such as Comma-Separated Values (CSV).  C:  Knows methods such as Direct Mapping of Relational Data to RDF (2012) for transforming data from the relational model (keys, values, rows, columns, tables) into RDF graphs.    B:  Versioning RDF data  B:  RDF data provenance  B:  Cleaning and reconciling RDF data  C:  Cleans a dataset by finding and correcting errors, removing duplicates and unwanted data.    B:  Mapping and enriching RDF data  C:  Uses available resources for named entity recognition, extraction, and reconciliation.",
            "title": "A: Creating and transforming Linked Data"
        },
        {
            "location": "/D2695955/#a-interacting-with-rdf-data",
            "text": "B:  Finding RDF data  C:  Knows relevant resources for discovering existing Linked Data datasets.  C:  Retrieves and accesses RDF data from the \"open Web\".  C:  Monitors and updates lists which report the status of SPARQL endpoints.  C:  Uses available vocabularies for dataset description to support their discovery.  C:  Registers datasets with relevant services for discovery.    B:  Processing RDF data using programming languages.  C:  Understands how components of the RDF data model (datasets, graphs, statements, and various types of node) are expressed in the RDF library of a given programming language by constructs such as object-oriented classes.  D:  Uses an RDF programming library to serialize RDF data in available syntaxes.  D:  Uses RDF-specific programming methods to iterate over components of RDF data.  D:  Uses RDF-library-specific convenience representations for common RDF vocabularies such as RDF, Dublin Core, and SKOS.    C:  Programatically associates namespaces to prefixes for use in serializing RDF or when parsing SPARQL queries.  D:  Uses RDF programming libraries to extract RDF data from CSV files, databases, or web pages.  D:  Uses RDF programming libraries to persistently stores triples in memory, on disk, or to interact with triple stores.  D:  Programatically infers triples using custom functions or methods.    C:  Understands how the pattern matching of SPARQL queries can be expressed using functionally equivalent constructs in RDF programming libraries.  D:  Uses RDF-specific programming methods to query RDF data and save the results for further processing.  D:  Uses utilities and convenience functions the provide shortcuts for frequently used patterns, such as matching the multiple label properties used in real data.  D:  Uses RDF libraries to process various types of SPARQL query result.      B:  Querying RDF data  C:  Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.  C:  Understands the basic syntax of a SPARQL query.  D:  Uses angle brackets for delimiting URIs.  D:  Uses question marks for indicating variables.  D:  Uses PREFIX for base URIs.    C:  Demonstrates a working knowledge of the forms and uses of SPARQL result sets (SELECT, CONSTRUCT, DESCRIBE, and ASK).  D:  Uses the SELECT clause to identify the variables to appear in a table of query results.  D:  Uses the WHERE clause to provide the graph pattern to match against the graph data.  D:  Uses variables in SELECT and WHERE clauses to yield a table of results.  D:  Uses ASK for a True/False result test for a match to a query pattern.  D:  Uses DESCRIBE to extract a single graph containing RDF data about resources.  D:  Uses CONSTRUCT to extract and transform results into a single RDF graph specified by a graph template.  D:  Uses FROM to formulate queries with URLs and local files.    C:  Understands how to combine and filter graph patterns using operators such as UNION, OPTIONAL, FILTER, and MINUS.  D:  Uses UNION to formulate queries with multiple possible graph patterns.  D:  Uses OPTIONAL to formulate queries to return the values of optional variables when available.  D:  Uses FILTER to formulates queries that eliminate solutions from a result set.  D:  Uses NOT EXISTS to limit whether a given graph pattern exists in the data.  D:  Uses MINUS to remove matches from a result based on the evaluation of two patterns.  D:  Uses NOT IN to restrict a variable to not being in a given set of values.    C:  Understands the major SPARQL result set modifiers, e.g., to limit or sort results, or to return distinct results only once.  D:  Uses ORDER BY to define ordering conditions by variable, function call, or expression.  D:  Uses DISTINCT to ensure solutions in the sequence are unique.  D:  Uses OFFSET to control where the solutions processed start in the overall sequence of solutions.  D:  Uses LIMIT to restrict the number of solutions processed for query results.  D:  Uses projection to transform a solution sequence into one involving only a subset of the variables.    C:  Understands the use of SPARQL functions and operators.  D:  Uses the regular expression (regex()) function for string matching.  D:  Uses aggregates to apply expressions over groups of solutions (GROUP BY, COUNT, SUM, AVG, MIN) for partitioning results, evaluating projections, and filtering.  D:  Uses the lang() function to return the language tag of an RDF literal.  D:  Uses the langMatches() function to match a language tag against a language range.  D:  Uses the xsd:decimal(expn) function to convert an expression to an integer.  D:  Uses the GROUP BY clause to transforms a result set so that only one row will appear for each unique set of grouping variables.  D:  Uses the HAVING clause to apply a filter to the result set after grouping.    C:  Differentiates between a Default Graph and a Named Graph, and formulates queries using the GRAPH clause.  D:  Formulates advanced queries using FROM NAMED and GRAPH on local data.  D:  Formulates advanced queries using FROM NAMED on remote data.  D:  Formulates advanced queries on data containing blank nodes.  D:  Formulates advanced queries using subqueries.    C:  Uses a temporary variable to extend a query.  C:  Understands the role of Property Paths and how they are formed by combining predicates with regular expression-like operators.  C:  Understands the concept of Federated Searches.  D:  Formulates advanced queries on a remote SPARQL endpoint using the SERVICE directive.  D:  Uses federated query to query over a local graph store and one or more other SPARQL endpoints.  D:  Pulls data from a different SPARQL endpoints in one single query using the SERVICE directive.    C:  Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by a third party tools and APIs.  C:  Reads and understands high-level descriptions of the classes and properties of a dataset in order to write queries.  C:  Uses available tools, servers, and endpoints to issue queries against a dataset.  D:  Execute SPARQL queries using the Jena ARQ command-line utility.  D:  Queries multiple local data files using ARQ.  D:  Uses ARQ to evaluate queries on local data.  D:  Uses Fuseki server to evaluate queries on a dataset.  D:  Queries multiple data files using Fuseki.  D:  Accesses DBPedia's SNORQL/SPARQL endpoint and issues simple queries.      B:  Visualizing RDF data  C:  Uses publicly available tools to visualize data.  D:  Uses Google FusionTables to create maps and charts.    C:  Distills results taken from large datasets so that visualizations are human-friendly.  C:  Converts/manipulates SPARQL query outputs (RDF-XML, JSON) to the exact format required by third party tools and APIs.    B:  Reasoning over RDF data  C:  Understands the principles and practice of inferencing.  C:  Uses common entailment regimes and understands their uses.  C:  Understands the role of formally declared domains and ranges for inferencing.  C:  Understands how reasoning can be used for integrating diverse datasets.  C:  Knows that Web Ontology Language (OWL) is available in multiple \"flavors\" that are variously optimized for expressivity, performant reasoning, or for applications involving databases or business rules.  C:  Understands that OWL Full supports all available constructs and is most appropriately used when reasoning performance is not a concern.    B:  Assessing RDF data quality  B:  RDF data analytics  C:  Uses available ontology browsing tools to explore the ontologies used in a particular dataset.    B:  Manipulating RDF data  C:  Knows the SPARQL 1.1 Update language for updating, creating, and removing RDF graphs in a Graph Store  D:  Uses INSERT/DELETE to update triples.  D:  Uses a CONSTRUCT query to preview changes before executing an INSERT/DELETE operation.    C:  Knows the SPARQL 1.1 Graph Store HTTP protocol for updating graphs on a web server (in \"restful\" style).  D:  Uses GET to retrieve triples from a default graph or a named graph.  D:  Uses PUT to insert set of triples into a new graph (or replace an existing graph).  D:  Uses DELETE to remove a graph.  D:  Uses POST to add triples to an existing graph.  D:  Uses proper syntax to request specific media types, such as Turtle.    C:  Understands the difference between SQL query language (which operates on database tables) and SPARQL (which operates on RDF graphs).",
            "title": "A: Interacting with RDF data"
        },
        {
            "location": "/D2695955/#a-creating-linked-data-applications",
            "text": "B:  Storing RDF data",
            "title": "A: Creating Linked Data applications"
        },
        {
            "location": "/structure/",
            "text": "Structure of the Competency Index\n\n\nThe Linked Data Competency Index exemplifies a broader class of documents for describing curriculum standards and learning objectives or outcomes.  There is no one standard way to formulate a competency index -- or competency framework, as they are sometimes called.  The range of subjects to be learned, and the requirements for learning, are too diverse to be normalized.  For the purposes of this index, the Competency Index Editorial Board devised the following structure:\n\n\nTopics\n.  A topic is a theme under which a set of competencies are grouped, such as \nDesigning RDF-based vocabularies\n and \nMaintaining RDF vocabularies\n.  Topics are grouped under higher-level topic clusters, such as \nRDF vocabularies and application profiles\n.  \n\n\nCompetencies\n.  A competency is a brief phrase characterizing knowledge (facts, insights, habits of mind, or skills) that may be learned.  Competencies may be used as building blocks for constructing self-learning plans, university courses, or even entire curricula.  Under the topic \nQuerying RDF Data\n, for example, one finds two competencies: \n\n\n\n\nUnderstands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.\n\n\nUnderstands the basic syntax of a SPARQL query.\n\n\n\n\nBenchmarks\n.  A benchmark is a brief phrase describing an action that can demonstrate accomplishment in a given competency.  If competencies are about learning, benchmarks are about doing.  Benchmarks may be used for devising homework assignments, exam questions, or self-testing checklists.  Under the competency \nUnderstands the basic syntax of a SPARQL query\n, for example, one finds three benchmarks:\n\n\n\n\nUses angle brackets for delimiting URIs.\n\n\nUses question marks for indicating variables.\n\n\nUses PREFIX for base URIs.\n\n\n\n\nBy design, the index does not classify competencies or benchmarks by level of difficulty and makes no assumptions about the background knowledge or skill set of learners.  This is because concepts that are comparatively easy for a library science student may be hard for a computer science student, and vice versa.  The index should also not be taken to imply an inherent order to the topics.",
            "title": "Structure of the Index"
        },
        {
            "location": "/structure/#structure-of-the-competency-index",
            "text": "The Linked Data Competency Index exemplifies a broader class of documents for describing curriculum standards and learning objectives or outcomes.  There is no one standard way to formulate a competency index -- or competency framework, as they are sometimes called.  The range of subjects to be learned, and the requirements for learning, are too diverse to be normalized.  For the purposes of this index, the Competency Index Editorial Board devised the following structure:  Topics .  A topic is a theme under which a set of competencies are grouped, such as  Designing RDF-based vocabularies  and  Maintaining RDF vocabularies .  Topics are grouped under higher-level topic clusters, such as  RDF vocabularies and application profiles .    Competencies .  A competency is a brief phrase characterizing knowledge (facts, insights, habits of mind, or skills) that may be learned.  Competencies may be used as building blocks for constructing self-learning plans, university courses, or even entire curricula.  Under the topic  Querying RDF Data , for example, one finds two competencies:    Understands that a SPARQL query matches an RDF graph against a pattern of triples with fixed and variable values.  Understands the basic syntax of a SPARQL query.   Benchmarks .  A benchmark is a brief phrase describing an action that can demonstrate accomplishment in a given competency.  If competencies are about learning, benchmarks are about doing.  Benchmarks may be used for devising homework assignments, exam questions, or self-testing checklists.  Under the competency  Understands the basic syntax of a SPARQL query , for example, one finds three benchmarks:   Uses angle brackets for delimiting URIs.  Uses question marks for indicating variables.  Uses PREFIX for base URIs.   By design, the index does not classify competencies or benchmarks by level of difficulty and makes no assumptions about the background knowledge or skill set of learners.  This is because concepts that are comparatively easy for a library science student may be hard for a computer science student, and vice versa.  The index should also not be taken to imply an inherent order to the topics.",
            "title": "Structure of the Competency Index"
        },
        {
            "location": "/style/",
            "text": "Writing effective competencies and benchmarks\n\n\n\n\n\n\n\n\nCompetencies (understanding)\n\n\nBenchmarks (doing)\n\n\n\n\n\n\n\n\n\n\nUnderstands\n\n\nUses\n\n\n\n\n\n\nKnows\n\n\nExpresses\n\n\n\n\n\n\nRecognizes\n\n\nDemonstrates\n\n\n\n\n\n\nDifferentiates ...\n\n\nConverts ...\n\n\n\n\n\n\n\n\n\n\n\n\nBegin each competency or benchmark with an action verb.  Competencies start with verbs related to understanding. Benchmarks start with verbs about doing.\n\n\n\n\n\n\nLimit each competency or benchmark to one sentence of circa 140 characters, the length of a Twitter posting.  If a thought seems to require more words, consider splitting it into two simpler thoughts.\n\n\n\n\n\n\nSpell out acronyms at least once.  Use your judgement.  In a competency index about Linked Data, frequently used acronyms such as \nOWL\n and \nURI\n need only be spelled out once: \nKnows Web Ontology Language, or OWL (2004), an RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).\n  \n\n\n\n\n\n\nInclude historical context if possible.  Linked Data technology has largely evolved over the past quarter century and continues to evolve.  Knowing the year when a technology or concept was introduced helps readers, for example: \nWorld Wide Web (1989)\n, \nHTTP (1991+)\n, \nURIs (1994)\n, \nOWL (2004)\n, and \nLinked Data (2006)\n.\n\n\n\n\n\n\nInclude enough detail to characterize the nature of competency in a domain.  Competencies and benchmarks should not aim at covering all features of a technology, in the manner of a reference manual.  Attempts at comprehensiveness risk making the CI brittle in the face of inevitable change, and they risk making the CI boring to read.\n\n\n\n\n\n\nDraw attention to ambiguity in the definition or use of terminology.  For example, one competency reads: \nKnows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain\n.\n\n\n\n\n\n\nEnlarge the set of topics covered by the index with prudence, bearing in mind that adding high-level topic cluster will broaden the scope of the index as a whole.  If new high-level topic clusters are proposed, however, consider whether they really fit into the scope of this particular competency index or, perhaps, imply the need for a second competency index with a different scope (for example, one focused on the design of Knowledge Organization Systems).",
            "title": "Style of the Index"
        },
        {
            "location": "/style/#writing-effective-competencies-and-benchmarks",
            "text": "Competencies (understanding)  Benchmarks (doing)      Understands  Uses    Knows  Expresses    Recognizes  Demonstrates    Differentiates ...  Converts ...       Begin each competency or benchmark with an action verb.  Competencies start with verbs related to understanding. Benchmarks start with verbs about doing.    Limit each competency or benchmark to one sentence of circa 140 characters, the length of a Twitter posting.  If a thought seems to require more words, consider splitting it into two simpler thoughts.    Spell out acronyms at least once.  Use your judgement.  In a competency index about Linked Data, frequently used acronyms such as  OWL  and  URI  need only be spelled out once:  Knows Web Ontology Language, or OWL (2004), an RDF vocabulary of properties and classes that extend support for expressive data modeling and automated inferencing (reasoning).       Include historical context if possible.  Linked Data technology has largely evolved over the past quarter century and continues to evolve.  Knowing the year when a technology or concept was introduced helps readers, for example:  World Wide Web (1989) ,  HTTP (1991+) ,  URIs (1994) ,  OWL (2004) , and  Linked Data (2006) .    Include enough detail to characterize the nature of competency in a domain.  Competencies and benchmarks should not aim at covering all features of a technology, in the manner of a reference manual.  Attempts at comprehensiveness risk making the CI brittle in the face of inevitable change, and they risk making the CI boring to read.    Draw attention to ambiguity in the definition or use of terminology.  For example, one competency reads:  Knows that the word \"ontology\" is ambiguous, referring to any RDF vocabulary, but more typically a set of OWL classes and properties designed to support inferencing in a specific domain .    Enlarge the set of topics covered by the index with prudence, bearing in mind that adding high-level topic cluster will broaden the scope of the index as a whole.  If new high-level topic clusters are proposed, however, consider whether they really fit into the scope of this particular competency index or, perhaps, imply the need for a second competency index with a different scope (for example, one focused on the design of Knowledge Organization Systems).",
            "title": "Writing effective competencies and benchmarks"
        },
        {
            "location": "/process/",
            "text": "Help us maintain the index\n\n\nTechnology continually evolves, and the index is a living document.  Do you see gaps in its coverage?  Proposing new competencies or benchmarks, updates, or even just changes of wording to the Competency Index Editorial Board is a fully guided process that begins with a \nsingle mouse click\n.\n\n\nBefore making that mouse click, you must be logged into an account on \nGithub\n, a superbly well-designed platform pioneered by developers of open-source software that increasingly is being used for collaboration on other types of project, such as this competency index.  \nJoining Github\n is free; you will be in excellent company.\n\n\nIf you \nlog into Github\n and navigate to the \nLinked Data Competency Index\n, you will see, in the upper right, a button labeled \nEdit on Github\n.  \nClicking this\n will present you with the option to \"fork\" the \nLDCI Github repository\n.  A copy of LDCI repository will be placed under your personal Github page and you will be placed directly into an editor where you can make changes to the competency index.\n\n\nThen type away! Add new competencies or tweak wordings.  Don't worry about the format; you are editing your own copy, so nothing will break.  When you are satisfied with your changes, give your suggestions a descriptive title, press the button to \"commit\" your changes.\n\n\n\n\nThen follow the prompts to create a \n\"pull request\"\n.  This will post your proposal to the LDCI Github repository with a request for consideration by the Competency Index Editorial Board.  The page created for your pull request may be used for follow-up questions and discussion.  Comments posted there will also be sent to the email address associated with your Github account.\n\n\nIf you merely want to raise issues related to the index, without proposing specific changes, you can do so by visiting the \nissues page\n and clicking on \nNew Issue\n.  This is the best way to raise general issues about the index, such as an extension of scope, or to put forward ideas that are not yet fully formed.\n\n\nMembers of the Editorial Board (and others with write access to the LDCI repository) should propose changes by creating a new branch of the repository, editing there, then \nissuing a pull request from the branch\n.",
            "title": "How to Contribute"
        },
        {
            "location": "/process/#help-us-maintain-the-index",
            "text": "Technology continually evolves, and the index is a living document.  Do you see gaps in its coverage?  Proposing new competencies or benchmarks, updates, or even just changes of wording to the Competency Index Editorial Board is a fully guided process that begins with a  single mouse click .  Before making that mouse click, you must be logged into an account on  Github , a superbly well-designed platform pioneered by developers of open-source software that increasingly is being used for collaboration on other types of project, such as this competency index.   Joining Github  is free; you will be in excellent company.  If you  log into Github  and navigate to the  Linked Data Competency Index , you will see, in the upper right, a button labeled  Edit on Github .   Clicking this  will present you with the option to \"fork\" the  LDCI Github repository .  A copy of LDCI repository will be placed under your personal Github page and you will be placed directly into an editor where you can make changes to the competency index.  Then type away! Add new competencies or tweak wordings.  Don't worry about the format; you are editing your own copy, so nothing will break.  When you are satisfied with your changes, give your suggestions a descriptive title, press the button to \"commit\" your changes.   Then follow the prompts to create a  \"pull request\" .  This will post your proposal to the LDCI Github repository with a request for consideration by the Competency Index Editorial Board.  The page created for your pull request may be used for follow-up questions and discussion.  Comments posted there will also be sent to the email address associated with your Github account.  If you merely want to raise issues related to the index, without proposing specific changes, you can do so by visiting the  issues page  and clicking on  New Issue .  This is the best way to raise general issues about the index, such as an extension of scope, or to put forward ideas that are not yet fully formed.  Members of the Editorial Board (and others with write access to the LDCI repository) should propose changes by creating a new branch of the repository, editing there, then  issuing a pull request from the branch .",
            "title": "Help us maintain the index"
        },
        {
            "location": "/board/",
            "text": "Competency Index Editorial Board\n\n\nTom Baker\n, chair \n\n\nDebbie Maron\n \n\n\nKai Eckert\n \n\n\nMagnus Pfeffer\n \n\n\nStuart Sutton\n \n\n\nLinks\n\n \nIssues\n\n\n \nPull requests",
            "title": "Editorial Board"
        },
        {
            "location": "/board/#competency-index-editorial-board",
            "text": "Tom Baker , chair   Debbie Maron    Kai Eckert    Magnus Pfeffer    Stuart Sutton    Links   Issues    Pull requests",
            "title": "Competency Index Editorial Board"
        },
        {
            "location": "/faq/",
            "text": "Frequently Asked Questions\n\n\n\n\n\n\nHow is the Linked Data Competency Index used in practice?\n  In the LD4PE Project, the index had two primary uses.  It provided a overview of the Linked Data field that teachers and learners could use to design university courses, training workshops, or self-directed learning programs.  It also provided a set of URIs, for use in metadata, for \ntagging tutorial resources\n in terms of topics or competencies covered.\n\n\n\n\n\n\nWhy are the competencies not numbered for easy reference?\n  The competency index will evolve over time.  Topics, competencies, or benchmarks could, in principle, be added anywhere in the index, which would change the numbers of any sequential numbering system.  Conceivably, we could decide to issue occasional stable releases.  In this case, the \nPython script for building the Markdown document\n might then be modified to calculate item numbers on the model of x (clusters), x.x (topics), x.x.x (competencies), x.x.x.x (benchmarks).  Moving to numbered releases would also help translators of the index (see the \ntranslation in Chinese\n.  If you feel strongly about this issue, please feel free to \nopen an issue\n in the Github issue tracker.\n\n\n\n\n\n\nAre the URIs for competencies covered by the DCMI Namespace Policy?\n The \nDCMI Namespace Policy\n lists several namespaces for which the organization makes a long-term commitment to their persistence and semantic stability.  Currently, all of those namespaces are managed under the \npurl.org\n resolver service, where DCMI administers its own PURL subdomain.  From the start of the LD4PE Project, the URIs for topics and competencies in the Linked Data Competency Index have been coined and managed under the \nAchievement Standards Network\n.  The topic \"Identity in RDF\", for example, is identified with the URI \nhttp://asn.desire2learn.com/resources/S2696001\n.  In order to extend its namespace policy to cover such URIs, DCMI would need to better understand ASN's policies with regard to persistence or, possibly, consider coining dublincore.org or PURL URIs under its own control.  More fundamentally, we should ask ourselves how important is it that the things in the index have persistent identity and stable semantics?  The answer should depend to an extent on how the competency index is used in practice.  We might start by distinguishing between topics (and topic clusters), which change infrequently and can be used in metadata as broad subject headings, and competencies (and benchmarks), which will be coined and superseded more quickly as technology evolves.  In the LD4PE Project, competency URIs were used to \ntag tutorial resources for retrieval by competency\n.  Many of these materials, such as blog postings, will become superseded or unavailable.  Do the competencies used as their tags really need to be more persistent than the described resources themselves?  If you feel strongly about this issue, please feel free to \nopen an issue\n in the Github issue tracker.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#frequently-asked-questions",
            "text": "How is the Linked Data Competency Index used in practice?   In the LD4PE Project, the index had two primary uses.  It provided a overview of the Linked Data field that teachers and learners could use to design university courses, training workshops, or self-directed learning programs.  It also provided a set of URIs, for use in metadata, for  tagging tutorial resources  in terms of topics or competencies covered.    Why are the competencies not numbered for easy reference?   The competency index will evolve over time.  Topics, competencies, or benchmarks could, in principle, be added anywhere in the index, which would change the numbers of any sequential numbering system.  Conceivably, we could decide to issue occasional stable releases.  In this case, the  Python script for building the Markdown document  might then be modified to calculate item numbers on the model of x (clusters), x.x (topics), x.x.x (competencies), x.x.x.x (benchmarks).  Moving to numbered releases would also help translators of the index (see the  translation in Chinese .  If you feel strongly about this issue, please feel free to  open an issue  in the Github issue tracker.    Are the URIs for competencies covered by the DCMI Namespace Policy?  The  DCMI Namespace Policy  lists several namespaces for which the organization makes a long-term commitment to their persistence and semantic stability.  Currently, all of those namespaces are managed under the  purl.org  resolver service, where DCMI administers its own PURL subdomain.  From the start of the LD4PE Project, the URIs for topics and competencies in the Linked Data Competency Index have been coined and managed under the  Achievement Standards Network .  The topic \"Identity in RDF\", for example, is identified with the URI  http://asn.desire2learn.com/resources/S2696001 .  In order to extend its namespace policy to cover such URIs, DCMI would need to better understand ASN's policies with regard to persistence or, possibly, consider coining dublincore.org or PURL URIs under its own control.  More fundamentally, we should ask ourselves how important is it that the things in the index have persistent identity and stable semantics?  The answer should depend to an extent on how the competency index is used in practice.  We might start by distinguishing between topics (and topic clusters), which change infrequently and can be used in metadata as broad subject headings, and competencies (and benchmarks), which will be coined and superseded more quickly as technology evolves.  In the LD4PE Project, competency URIs were used to  tag tutorial resources for retrieval by competency .  Many of these materials, such as blog postings, will become superseded or unavailable.  Do the competencies used as their tags really need to be more persistent than the described resources themselves?  If you feel strongly about this issue, please feel free to  open an issue  in the Github issue tracker.",
            "title": "Frequently Asked Questions"
        }
    ]
}